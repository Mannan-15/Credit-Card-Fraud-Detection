# -*- coding: utf-8 -*-
"""CCF_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OO_fL1ZqNXarrzgkLJDK-0EXYSyTkUH-

__<font size = 5>Data Exploration__
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import warnings
warnings.filterwarnings("ignore")

df = pd.read_csv('creditcard.csv')
df.head()

df['V22'].replace('-', np.nan, inplace = True)
df['V22'] = df['V22'].astype('float64')

df.info()

df.fillna(method = 'ffill', inplace = True)
df.info()

df.shape

df.describe()

"""__<font size = 5>Data Visualization__"""

sns.countplot(df, x = 'Class')
plt.grid(axis = 'y', linestyle = '--', color = 'grey')
plt.show()

plt.figure(figsize = (15,6))
# mask = np.triu(np.ones_like(df.corr(), dtype=bool))
# mask = np.tril(np.ones_like(df.corr(), dtype=bool))
sns.heatmap(df.corr(), fmt = '.2f', cmap = 'coolwarm')
plt.tight_layout()
plt.show()

sns.kdeplot(df, x = 'Amount', fill = 'blue')
plt.show()

df['log_amount'] = np.log1p(df['Amount'])
sns.kdeplot(df, x = 'log_amount', fill = 'blue')
plt.show()

df.groupby('Class').mean()

legit = df[df['Class'] == 0]
fraud = df[df['Class'] == 1]
legit_sample = legit.sample(n=103*2)
new_df = pd.concat([legit_sample, fraud], axis=0)
new_df.head()

new_df.drop(columns = ['Time'], inplace = True)

new_df.hist(figsize = (15,15))
plt.tight_layout()
plt.show()

new_df['Class'].value_counts()

new_df.groupby('Class').mean()

from sklearn.preprocessing import StandardScaler
X = new_df.drop(columns = 'Class', axis = 1)
y = new_df['Class']
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_scaled = pd.DataFrame(X_scaled, columns = X.columns)

"""__<font size = 5>Model -> Logistic Regression__"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.1, random_state=3)

xtrain.shape, xtest.shape

log_reg = LogisticRegression()
log_reg.fit(xtrain, ytrain)

ypred_log = log_reg.predict(xtest)
accuracy_score(ytest, ypred_log)

"""__<font size = 5>Model -> Decision Trees__"""

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier()
dt.fit(xtrain, ytrain)

ypred_dt = dt.predict(xtest)
accuracy_score(ytest, ypred_dt)

from sklearn import tree

plt.figure(figsize = (20, 25))
tree.plot_tree(dt,
               feature_names = xtrain.columns,
               class_names = ['Class 0', 'Class 1'],
               rounded = True,
               proportion = True,
               filled = True)
plt.show()

"""__<font size = 5>Model -> KNN__"""

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(xtrain, ytrain)

ypred_knn = knn.predict(xtest)
accuracy_score(ytest, ypred_knn)

error = []
for i in range(1, 40) :
  m = KNeighborsClassifier(n_neighbors=i)
  m.fit(xtrain, ytrain)
  ypred_m = m.predict(xtest)
  error.append(np.mean(ypred_m != ytest))

plt.figure(figsize = (10,8))
plt.plot(range(1,40), error, marker = 'o', linestyle = 'dashed', color = 'b', markerfacecolor = 'red', markersize = 10)
plt.title('Error Rate V/s K Value')
plt.xlabel('K Value')
plt.ylabel('Mean Error')
plt.show()

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
con_mat = confusion_matrix(ytest, ypred_knn)
ConfusionMatrixDisplay(con_mat, display_labels = [True, False]).plot()
plt.show()

from sklearn.pipeline import Pipeline
model_pipeline = Pipeline([
    ('scaler', scaler),
    ('model', knn)
])

model_pipeline.predict(xtest.iloc[:1, :])
